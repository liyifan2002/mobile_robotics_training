{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 - Visual Features and RANSAC\n",
    "\n",
    "In this exercise, you will implement a visual feature detector and descriptor to find correspondences/matches between two images together. You will then use these matches to stich these images together within a RANSAC routine to deal with outliers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Implement a feature detector [5.0]\n",
    "\n",
    "Your goal is to implement a feature detector as explained in the lecture. Implement both the Harris-Corner detector and Shi-Tomasi corner criteria  in the function. Implement the ``compute_corners`` function. It takes as input:\n",
    "\n",
    "    I : float [MxN] \n",
    "        grayscale image\n",
    "\n",
    "    type :  string\n",
    "            corner type ('harris' or 'Shi-Tomasi')\n",
    "\n",
    "    T:  float\n",
    "        threshold for corner detection\n",
    "    \n",
    "and returns:\n",
    "\n",
    "    corners : numpy array [num_corners x 2] \n",
    "              Coordinates (x,y) of the detected corners.    \n",
    "\n",
    "Test your implementation on the 'checkerboard.jpg' image and verify that the detections are correct.\n",
    "\n",
    "** Hints **: \n",
    "\n",
    "1. You may use 'ndimage.convolve' function from the scipy library to perform 2D convolution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import ex3 as ex\n",
    "\n",
    "# read images and transform them to grayscale\n",
    "I_checkerboard = imageio.imread('checkerboard.png')\n",
    "\n",
    "# compute corners and visualize them on top of the image\n",
    "T_harris = 1  # TODO:  Choose a suitable threshold\n",
    "T_shi_tomasi = 1 # TODO:  Choose a suitable threshold\n",
    "corners_harris = ex.compute_corners(I_checkerboard, 'harris', T_harris)\n",
    "corners_shi_tomasi = ex.compute_corners(I_checkerboard, 'shi-tomasi', T_shi_tomasi)\n",
    "\n",
    "# Visualize the detections by plotting them over the image\n",
    "fig = plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(I_checkerboard, cmap='gray')\n",
    "plt.plot(corners_harris[:,0], corners_harris[:,1], 'rx', markersize=12)\n",
    "plt.xlabel(\"Harris Corners\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(I_checkerboard, cmap='gray')\n",
    "plt.plot(corners_shi_tomasi[:,0], corners_shi_tomasi[:,1], 'rx', markersize=12)\n",
    "plt.xlabel(\"Shi-Tomasi Corners\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Compute a feature descriptor [5.0]\n",
    "\n",
    "The next task is to implement a feature descriptor as explained in the lecture. The descriptor for each keyppoint should 128 bits. Implement the ``compute_descriptors`` function. It takes as input:\n",
    "\n",
    "    I : float [MxN]\n",
    "        grayscale image as a 2D numpy array\n",
    "    corners : numpy array [num_corners x 2] \n",
    "              Coordinates of the detected corners. \n",
    "    \n",
    "and returns:\n",
    "\n",
    "    D : numpy array [num_corners x 128]\n",
    "        128 bit descriptors  corresponding to each corner keypoint\n",
    "\n",
    "Test your implementation on the 'checkerboard.jpg' image.\n",
    "\n",
    "** Hints **: \n",
    "\n",
    "1. You may use think of re-using the gradient information from ``compute_corners`` for computing descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read images and transform them to grayscale\n",
    "I1 = imageio.imread('mountain_1.jpg')\n",
    "I1_gray = cv2.cvtColor(I1, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "I2 = imageio.imread('mountain_2.jpg')\n",
    "I2_gray = cv2.cvtColor(I2, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# compute corner keypoints\n",
    "C1_harris =  ex.compute_corners(I1_gray, 'harris', T_harris)\n",
    "C2_harris =  ex.compute_corners(I2_gray, 'harris', T_harris)\n",
    "\n",
    "C1_shi_tomasi =  ex.compute_corners(I1_gray, 'shi-tomasi', T_harris)\n",
    "C2_shi_tomasi =  ex.compute_corners(I2_gray, 'shi-tomasi', T_harris)\n",
    "\n",
    "# compute the descriptor for the two images \n",
    "D1_harris = ex.compute_descriptors(I1_gray, C1_harris)\n",
    "D2_harris = ex.compute_descriptors(I2_gray, C2_harris)\n",
    "\n",
    "D1_shi_tomasi = ex.compute_descriptors(I1_gray, C1_shi_tomasi)\n",
    "D2_shi_tomasi = ex.compute_descriptors(I2_gray, C2_shi_tomasi)\n",
    "\n",
    "C1 = np.array([[10, 10], [50, 50], [100, 100]])\n",
    "C2 = np.array([[10, 10], [50, 50], [100, 100]])\n",
    "M = np.array([[0,0],[1,1],[2,2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Compute correspondences for the images [5.0]\n",
    "\n",
    "Now, the goal is to compute correspondences (matches) between two images based on the descriptors computed in the previous task. The descriptors can be compared by computing a score in a one vs all fashion, and a match should be accepted if it meets the Lowe's critera as described in the lecture. \n",
    "\n",
    "Implement the function `compute_matches`. It takes as input:\n",
    "    \n",
    "    D1 : numpy array [num_corners x 128]\n",
    "         descriptors for image 1 keypoints\n",
    "    \n",
    "    D2 : numpy array [num_corners x 128]\n",
    "         descriptors for image 2 keypoints\n",
    "\n",
    "and returns\n",
    "\n",
    "    M : numpy array [num_matches x 2]\n",
    "        [cornerIdx1, cornerIdx2] each row contains indices of corresponding keypoints from each image.\n",
    "\n",
    "Test your implementation on the images used in the previous task and visualize the matches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matches for harris corner keypoints and corresponding keypoints\n",
    "M_harris = ex.compute_matches(D1, D2)\n",
    "\n",
    "# Matches for Shi-Tomasi keypoints and corresponding keypoints\n",
    "M_shi_tomasi = ex.compute_matches(D1, D2)\n",
    "\n",
    "# Visualize the matches\n",
    "plt.figure()\n",
    "ex.plot_matches(I1_gray, I2_gray, C1_harris, C2_harris, M_harris)\n",
    "\n",
    "plt.figure()\n",
    "ex.plot_matches(I1_gray, I2_gray, C1_shi_tomasi, C2_shi_tomasi, M_shi_tomasi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Application: Image Stiching [5.0]\n",
    "\n",
    "In this part of the assignment, the goal is to stitch two images (which we have used in the previous tasks) together. To stitch these images, we first need to compute a transformation (also called as Homography) between these two images. We will use the matches computed in the previous step to compute the homography.\n",
    "\n",
    "The matches computed by matching the descriptors usually contain outliers which are bad correspondences. In this task, we want to estimate the transformation (Homography) between the two image despite the presence of outliers in th matches. In order to achieve this, you have to implement a RANSAC based algorithm which estimates the homography and the set of inlier matches. \n",
    "\n",
    "In order to implement the algorithm, you will need to: \n",
    "  - For each iteration, randomly choose four feature matches in all found matches in function **compute_homography_ransac()**;\n",
    "  - Estimate a homography using 4 corresponding points in two images in function **comute_homography_four_matches(). This function has already been implemented for you. \n",
    "  - Calculate the geometric distance (error) betweeen the transformed points from first image and the corresponding points in the second image, namely residuals, in function **compute_residual()**;\n",
    "  - Check all matches with estimated homography matrix and label inliers based on the 'max_inlier_thres' in function **compute_homography_ransac()**;\n",
    "  - Terminate the procedure if a homography is found which satisfies the inliers codition or exceeds the maximum number iterations the in function **compute_homography_ransac()**.\n",
    "\n",
    "Refer to the lecture slides for all the details.  After you have implemented all the above, you should have an good estimated homography matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Estimate Homography matrix and set of inlier matches and visualize the inlier matches.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Application: Image stiching [5.0]\n",
    "\n",
    "Use the estimated homography matrix to stitch and plot two images in one figure.\n",
    "\n",
    "**Hint:** The **warpPerspective** functions of **OpenCV** library can be useful in this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Source:\n",
    "\"Github: daeyun Image-Stitching Test Images\", 2016. [Online]. Available: https://github.com/daeyun/Image-Stitching/tree/master/img/hill."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
